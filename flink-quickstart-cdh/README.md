# Flink Quickstart Application

The purpose of the Flink Quickstart Application is to provide a self-contained boilerplate code example for a Flink application on top of CDH. The application demonstrates some basic capabilities of the DataStream API. 
It collects basic heap statistics of the JVM it is running on and dumps the collected data to a filesystem. It also triggers alert messages when reaching concerning heap values. The application purposefully leaks memory to generate changing data values.

## Usage
Check out the repository and build the artifact:
```
git clone https://github.infra.cloudera.com/morhidi/flink-ref.git
cd flink-quickstart-cdh
mvn clean install
```

## Importing the project to IntelliJ
The quickstart application is based on the upstream Flink quickstart maven archetype. The project can be imported into IntelliJ by following the instructions from the public Flink documentation:
https://ci.apache.org/projects/flink/flink-docs-stable/dev/projectsetup/java_api_quickstart.html#maven

## Running the application from IntelliJ
Simply run the class HeapMonitorPipeline from the IDE which should print one or multiple lines to the console (depending on the number of cores of your machine chosen as default parallelism):
```
...
13:50:54,524 INFO  com.cloudera.streaming.examples.flink.HeapMonitorSource       - starting HeapMonitorSource
13:50:54,524 INFO  com.cloudera.streaming.examples.flink.HeapMonitorSource       - starting HeapMonitorSource
13:50:54,524 INFO  com.cloudera.streaming.examples.flink.HeapMonitorSource       - starting HeapMonitorSource
13:50:54,524 INFO  com.cloudera.streaming.examples.flink.HeapMonitorSource       - starting HeapMonitorSource
...
```

The heap statistics are generated by the HeapMonitorSource class, a custom source implementation. All messages are saved to the filesystem, local or HDFS, depending on where the application runs. The output path is configurable with a program argument, e.g.:
```
--output /tmp/flinf-quickstart-cdh/stats
--output hdfs:///tmp/flinf-quickstart-cdh/stats
```

Under normal circumstances the logs are silent. The application triggers alert events only when the old gen space of the heap exceeds certain threshold values. The heap alerts are sent to a special sink called LogSink:

```
13:50:56,481 INFO  com.cloudera.streaming.examples.flink.LogSink                 - HeapAlert{message='Full GC expected soon', triggeringStats=HeapStats{area=PS Old Gen, used=65709552, max=5726797824, ratio=0.011474047804625276, jobId=3, hostname='morhidi-mbp.local'}}
13:50:56,482 INFO  com.cloudera.streaming.examples.flink.LogSink                 - HeapAlert{message='Full GC expected soon', triggeringStats=HeapStats{area=PS Old Gen, used=65709552, max=5726797824, ratio=0.011474047804625276, jobId=11, hostname='morhidi-mbp.local'}}
13:50:56,481 INFO  com.cloudera.streaming.examples.flink.LogSink                 - HeapAlert{message='Full GC expected soon', triggeringStats=HeapStats{area=PS Old Gen, used=65709552, max=5726797824, ratio=0.011474047804625276, jobId=10, hostname='morhidi-mbp.local'}}
```

LogSink is a custom sink implementation that simply sends the messages to the logging framework. The logs can be redirected via log4j to any centralized logging system or simply printed to the standard output when debugging. The quick start application provides a sample log4j config for redirecting the logs to the standard error.

```
...
log4j.logger.com.cloudera=INFO, stdout, kafka
log4j.additivity.com.cloudera=false

log4j.appender.kafka=org.apache.kafka.log4jappender.KafkaLog4jAppender
log4j.appender.kafka.brokerList=flink-ref-1.gce.cloudera.com:9092,flink-ref-2.gce.cloudera.com:9092,flink-ref-3.gce.cloudera.com:9092
log4j.appender.kafka.topic=flink
log4j.appender.kafka.layout=org.apache.log4j.PatternLayout
log4j.appender.kafka.layout.ConversionPattern=%d{HH:mm:ss,SSS} %-5p %-60c %x - %m%n
...
```
The alerts thresholds are configurable via two program arguments that can be set to a low value for testing:

```
--warningThreshold 0.1
--criticalThreshold 0.2
```

## Testing the application from IntelliJ
The business logic of a Flink application consists of one or more operators chained together, which is often called a pipeline. Pipelines can be extracted to static methods and can be easily tested with JUnit framework. The HeapMonitorPipelineTest class gives a sample for this.

## Running the application on a remote Cluster
The Flink Quickstart Application can be deployed on a CDH cluster remotely. The actual version of the application was tested agains CDH6.2.x and FLINK-1.8.1-cdh6.2.0-p1-el7 without any security integration on it. The Flink parcel is accessible at
https://drive.google.com/drive/folders/1k_UfOVdDHvcfHIwT4L03xBEmF6DGLdwQ

Uploading the application:
```
scp target/flink-quickstart-cdh-1.0-SNAPSHOT.jar root@flink-ref-1.gce.cloudera.com:. 
```

Running the application
```
JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera/ flink run -sae -m yarn-cluster -p 2 -c com.cloudera.streaming.examples.flink.HeapMonitorPipeline flink-quickstart-cdh-1.0-SNAPSHOT.jar --output hdfs:///tmp/flink-quickstart-cdh/alerts

```

After launching the application Flink will create a log running yarn session and launch a dashbord where the application can be monitored. The Flink dashbord can be reached from CM through the following path:
Cluster->Yarn->Applications->application_<ID>->Tracking URL:	ApplicationMaster. 

Log messages from a Flink application can be also collected and forwarded to a Kafka topic for convenience. This requires only a few extra configuration steps and dependencies in Flink. The default log4j config can be overriden with a command parameter:

```
-yD log4j.configuration.file=log4j.properties
```

```
log4j.rootLogger=DEBUG, file

log4j.logger.akka=INFO
log4j.logger.org.apache.kafka=INFO
log4j.logger.org.apache.hadoop=INFO
log4j.logger.org.apache.zookeeper=INFO

log4j.appender.file=org.apache.log4j.FileAppender
log4j.appender.file.file=${log.file}
log4j.appender.file.append=false
log4j.appender.file.layout=org.apache.log4j.PatternLayout
log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
log4j.logger.org.apache.flink.yarn.Utils=DEBUG
log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file

log4j.logger.com.cloudera=INFO, stdout, kafka
log4j.additivity.com.cloudera=false

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.Target   = System.out
log4j.appender.stdout.layout.ConversionPattern=%d{HH:mm:ss,SSS} %-5p %-60c %x - %m%n

log4j.appender.kafka=org.apache.kafka.log4jappender.KafkaLog4jAppender
log4j.appender.kafka.brokerList=flink-ref-1.gce.cloudera.com:9092,flink-ref-2.gce.cloudera.com:9092,flink-ref-3.gce.cloudera.com:9092
log4j.appender.kafka.topic=flink
log4j.appender.kafka.layout=org.apache.log4j.PatternLayout
log4j.appender.kafka.layout.ConversionPattern=%d{HH:mm:ss,SSS} %-5p %-60c %x - %m%n

```

The KafkaLog4jAppender requires a few dependencies also which can be shipped with the run command also. The --yarnship parameter should point to a local folder containing all the dependencies for logging:
```
--yarnship kafka-appender

[root@flink-ref-1 ~]# tree kafka-appender
kafka-appender
├── kafka-clients-2.1.0-cdh6.2.0.jar
└── kafka-log4j-appender-0.9.0.0.jar

```

An example for the full command with Kafka logging:
```
JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera/ flink run -sae -m yarn-cluster -p 2 --yarnship kafka-appender -yD log4j.configuration.file=log4j.properties -c com.cloudera.streaming.examples.flink.HeapMonitorPipeline flink-quickstart-cdh-1.0-SNAPSHOT.jar --ship kafka-appender --output hdfs:///tmp/flink-quickstart-cdh/alerts
```

Accessing the logs from the Kafka topic is possible then with:
```
kafka-console-consumer --bootstrap-server flink-ref-1.gce.cloudera.com:9092 --topic flink
```





